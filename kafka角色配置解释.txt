相关配置解释：

producer:
1、producer的配置不需要zookeeper地址，会直接获取kafka的元数据，直接和broker进行通信
2、ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG即value.serializer，kafka生产者与broker之间数据是以byte进行传递的，所以这个参数的意思是把我们传入对象转换成byte[]的类，一般使用org.apache.kafka.common.serialization.ByteArraySerializer即可，我们自己把对象序列化为byte[]
3、ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG即key.serializer，首先说明下key值是干什么的，若我们指定了key的值，生产者则会根据该key进行hash散列计算出具体的partition。若不指定，则随机选择partition。一般情况下我们没必要指定该值。这个类与上面功能一样，即将key转换成byte[]
4、ProducerConfig.LINGER_MS_CONFIG即linger.ms，为了减少请求次数提高吞吐率，这个参数为每次提交间隔的次数，若设置了该值，如1000，则意味着我们的消息可能不会马上提交到kafka服务器，需要等上1秒中，才会进行批量提交。我们可以适当的配置该值。0为不等待立刻提交。
consumer：
1、zookeeper.connect：zookeeper的地址，多个之间用，分割
2、group.id：这个值可以随便写，但建议写点有意义的值，别随便写个123。kafka保证同一个组内的消息只会被消费一次，若需要重复消费消息，则可以配置不同的groupid。
3、auto.commit.interval.ms：consumer自己会记录消费的偏移量，并定时往zookeeper上提交，该值即为提交时间间隔，若该值设置太大可能会出现重复消费的情况，如我们停止了某个consumer，但该consumer还未往zookeeper提交某段时间的消费记录，这导致我们下次启动该消费者的时候，它会从上次提交的偏移量进行消费，这就导致了某些数据的重复消费。
注意：在杀死consumer进程后，应等一会儿再去重启，因为杀死consumer进程时，会删除zookeeper的一些临时节点，若我们马上重启的话，可能会在启动的时候那些节点还没删除掉，出现写不必要的错误